
---
  title: Simple Text Classification with Python's NLTK
  date: 2015-12-21 21:51:28
  draft: false
---
<p>
  Last Tuesday I had a very interesting talk with the CTO's and Engineering leads of
the SocialAtom Ventures. The idea was to talk about the current challenges everybody
is facing and share the experiences we've had with similar problems. One of
the companies (<a href="http://www.bankity.com/">Bankity</a>) is struggling to
separate bogus messages from their transaction messages. This is currently being done
through Google's Prediction API which not only makes them dependent on a vendor, but
also is a black-box that is hard to reason about.
</p>

<p>
  Early startups tend to be very wary regarding topics that sound too much like
  Rocket Science. There is of course some appeal to avoid complexity when your
  startup is on an early stage. Machine learning is one of those topics everybody
  feels intimidated about. In this post I'll try to convince you that is fine to
  experiment. And the results/effort might surprise you.
</p>

<h2>Disclaimer</h2>

<p>This is actually the first time I do text classification. I'm  happy with the
current results and I've learned a lot. I felt this post could motivate some people
to experiment with NLTK. I'm not an authority on the topic.
</p>

<h2>Text Classification Basics</h2>

<p>Classification problems are those that try to find the most suitable category for
a given object. An example you are probably familiar with is your e-mail spam
detector. The classification algorithm behind it is able to classify the email
(objects) into the "spam" or "ham" categories.
</p><p>
</p><p>In order to do the classification, a subset of characteristics (features) from the text is selected. These "features" are the high-level inputs that our classifier will receive.
</p><p><h2>Getting Started With Python NLTK</h2>
</p><p><a href="http://www.nltk.org/">NLTK</a> stands for "Natural Language Toolkit". Installing it in your environment using <strong>pip</strong> is as simple as:
</p>

{{<highlight bash>}}
$ pip install nltk
{{</highlight>}}

<p>
</p><p>Once this is done, we can start preparing our data to train our first classifier. I got all my data as a spreadsheet that I saved in CSV with the following format:
</p><p>

{{<highlight text >}}
// CSV FILE EXAMPLE
message,isTransaction
"Transaction Approved, $15.700, Hora 03:40, Comercio PAYU NETFLIX COM",1
"I called you 20/10 el **693 at 10:11.",0
"You received 2 calls from: *198 at 20/10/15 This is a free service of Claro.",0
// [...]
{{</highlight>}}
<p>
Parsing de data in Python is very easy using the csv library.
</p>

{{<highlight python >}}
# A function to tokenize the input into words.
def tokenize(element):
    # tokens = nltk.tokenize_words(transactions[0])
    # return nltk.word_tokenize(element)
    return split(&quot;\W+&quot;, element)

def load_dataset():
    transactions = []
    other = []
    all_words = []
    with open('dataset2.csv') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            words = tokenize(row[&quot;message&quot;])
            all_words.extend(words)
            if(row["isTransaction"] == "1";):
                transactions.append(words)
            else:
                other.append(words)
    return(transactions, other, all_words)

transactions, other, all_words = load_dataset()
{{</highlight>}}

</p><p>
</p><p>NL often requires that you do some simple pre-processing of the input. Although I used a simple regex for tokenizing the input into words, nltk comes with its own tokenizer (as shown in the tokenizer function from the snippet).
</p><p><h2>Training our first classifier</h2>
</p><p>The text-book example on text classification uses <a href="https://en.wikipedia.org/wiki/Bayes_classifier">Bayes</a> to create a probability model that will use the frequency of occurrences in order to create a statistical model to infer the class of a given object. The "features" in this case correspond to the presence of a given word in the document.
</p><p><h4>A note on dimensionality</h4>
</p><p>Each feature creates a "dimension" the classifier will have to deal with. The more dimensions the machine learning algorithm has, the more processing power it will require for analysing and training. Some features do not provide significant information, so it's often useful to spend some time looking at the features gathered from the documents processed and see their histogram of occurrences.
</p>

{{<highlight python >}}

frequencies = nltk.FreqDist(w.lower() for w in all_words)
print(frequencies.most_common(50))

{{</highlight>}}


<p>Intuitively, features that are too rare (words that happen only once in the entire dataset) do not provide general information. Features that might be too common (stop words as in 'the', 'a', etc) are too broad to be used in classification decisions. Dimensionality Reduction (or in lay terms, selecting the best and most meaningful features) can make your algorithm converge faster.
</p>

<h2>Naive Bayes Classifier</h2>

<p>Once you have your dataset ready, is time to train your model. Split your dataset
into "training" and
"test". T<a href="http://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio/13623707#13623707">his
article of StackOverflow</a> provides some good practices handling your dataset.
</p><p>
{{<highlight python >}}
# Convert from frequencies to 'contains word' feature
def document_features(words, word_features):
    document_words = set(words)
    features = {}
    for word in word_features:
    features[word] = (word in document_words)
    return features

# Label all the dataset
transaction_features = [(document_features(t, word_features), 'transaction') for t in transactions]
other_features = [(document_features(o, word_features), 'other') for o in other]

# Select all the feature tests.
feature_sets = transaction_features + other_features

# Shuffle the dataset per test
random.shuffle(feature_sets)

# In our case we use 1000 items for training, 100 for testing.
train_set, test_set = feature_sets[:1000], feature_sets[100:]

real_labels = [label for (_,label) in test_set]
predictions = classifier.classify_many(x for (x,_) in test_set)
valid_predictions = [label == prediction for (label, prediction) in zip(real_labels, predictions)]

accuracy = valid_predictions.count(True) / float(len(valid_predictions))
{{</highlight>}}

<p>That's it. We are measuring the <strong>Avg-Error</strong> in this case as a simple benchmark of convergence of the algorithm. After running the algorithm a couple of thousands of times and playing with the parameters I got to an ~84.3% Avg-Error. I noticed that the convergence rate did not change dramatically by using the top 300 features of the dataset.
</p>

<h2>Trying other Classifiers</h2>

<p>If the Bayes classifier didn't
work, <a href="http://www.nltk.org/howto/classify.html">there are more classifiers
available in NLTK.</a>. In my case the ~84.3% from Bayes wasn't enough. NLTK's
classifiers conform to the same interface. Because of this, running the same
test-program and checking different classifiers to see which performs better is super
easy!
</p>

{{<highlight python >}}
classifier = nltk.classify.DecisionTreeClassifier.train(train_set,entropy_cutoff=0,support_cutoff=0)
{{</highlight>}}

<p>
  In my case, the DecisionTreeClassifier, was the most performant. Using only 1000
  examples for training and 100 test objects, it was easy to reach a ~97% accuracy.
</p>

<h2>Conclusions</h2>
<p>NLTK is a very simple library for Natural Language processing in Python. It takes
a couple of hundred of lines to check several classifiers and determine which one
works best for your dataset. Tuning the features of your given problem might improve
the processing costs of the algorithm or even improve the convergence rate.</p>
